{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#importing basic packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scraping_data import crawl_main\n",
    "from ex_feaures import ex_features_main"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Loading the scraped data\n",
    "data0 = pd.read_csv('urldata.csv')\n",
    "data0.head()\n",
    "\n",
    "#Checking the shape of the dataset\n",
    "data0.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Listing the features of the dataset\n",
    "data0.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Information about the dataset\n",
    "data0.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **4. Visualizing the data**\n",
    "Few plots and graphs are displayed to find how the data is distributed and the how features are related to each other."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Plotting the data distribution\n",
    "data0.hist(bins = 50,figsize = (15,15))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Correlation heatmap\n",
    "\n",
    "plt.figure(figsize=(15,13))\n",
    "sns.heatmap(data0.corr())\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **5. Data Preprocessing & EDA**\n",
    "Here, we clean the data by applying data preprocesssing techniques and transform the data to use it in the models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data0.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above obtained result shows that the most of the data is made of 0's & 1's except 'Domain' & 'URL_Depth' columns. The Domain column doesnt have any significance to the machine learning model training. So dropping the *'Domain'* column from the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Dropping the Domain column\n",
    "data = data0.drop(['Domain'], axis = 1).copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This leaves us with 16 features & a target column. The *'URL_Depth'* maximum value is 20. According to my understanding, there is no necessity to change this column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#checking the data for null or missing values\n",
    "data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the feature extraction file, the extracted features of legitmate & phishing url datasets are just concatenated without any shuffling. This resulted in top 5000 rows of legitimate url data & bottom 5000 of phishing url data.\n",
    "\n",
    "To even out the distribution while splitting the data into training & testing sets, we need to shuffle it. This even evades the case of overfitting while model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# shuffling the rows in the dataset so that when splitting the train and test set are equally distributed\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above execution, it is clear that the data doesnot have any missing values.\n",
    "\n",
    "By this, the data is throughly preprocessed & is ready for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **6. Splitting the Data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sepratating & assigning features and target columns to X & y\n",
    "y = data['Label']\n",
    "X = data.drop('Label',axis=1)\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets: 80-20 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2, random_state = 12)\n",
    "X_train.shape, X_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **7. Machine Learning Models & Training**\n",
    "\n",
    "From the dataset above, it is clear that this is a supervised machine learning task. There are two major types of supervised machine learning problems, called classification and regression.\n",
    "\n",
    "This data set comes under classification problem, as the input URL is classified as phishing (1) or legitimate (0). The supervised machine learning models (classification) considered to train the dataset in this notebook are:\n",
    "* Decision Tree\n",
    "* Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating holders to store the model performance results\n",
    "ML_Model = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "\n",
    "#function to call for storing the results\n",
    "def storeResults(model, a,b):\n",
    "  ML_Model.append(model)\n",
    "  acc_train.append(round(a, 3))\n",
    "  acc_test.append(round(b, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **7.1. Decision Tree Classifier**\n",
    " Decision trees are widely used models for classification and regression tasks. Essentially, they learn a hierarchy of if/else questions, leading to a decision. Learning a decision tree means learning the sequence of if/else questions that gets us to the true answer most quickly.\n",
    "\n",
    "In the machine learning setting, these questions are called tests (not to be confused with the test set, which is the data we use to test to see how generalizable our model is). To build a tree, the algorithm searches over all possible tests and finds the one that is most informative about the target variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Decision Tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# instantiate the model\n",
    "tree = DecisionTreeClassifier(max_depth = 5)\n",
    "# fit the model\n",
    "tree.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "y_test_tree = tree.predict(X_test)\n",
    "y_train_tree = tree.predict(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Performance Evaluation:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#computing the accuracy of the model performance\n",
    "acc_train_tree = accuracy_score(y_train,y_train_tree)\n",
    "acc_test_tree = accuracy_score(y_test,y_test_tree)\n",
    "\n",
    "print(\"Decision Tree: Accuracy on training Data: {:.3f}\".format(acc_train_tree))\n",
    "print(\"Decision Tree: Accuracy on test Data: {:.3f}\".format(acc_test_tree))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#checking the feature improtance in the model\n",
    "plt.figure(figsize=(9,7))\n",
    "n_features = X_train.shape[1]\n",
    "plt.barh(range(n_features), tree.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_features), X_train.columns)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Storing the results:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "#Caution: Execute only once to avoid duplications.\n",
    "storeResults('Decision Tree', acc_train_tree, acc_test_tree)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **7.2. Random Forest Classifier**\n",
    "Random forests for regression and classification are currently among the most widely used machine learning methods.A random forest is essentially a collection of decision trees, where each tree is slightly different from the others. The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data.\n",
    "\n",
    "If we build many trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. To build a random forest model, you need to decide on the number of trees to build (the n_estimators parameter of RandomForestRegressor or RandomForestClassifier). They are very powerful, often work well without heavy tuning of the parameters, and don’t require scaling of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate the model\n",
    "forest = RandomForestClassifier(max_depth=5)\n",
    "\n",
    "# fit the model\n",
    "forest.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#predicting the target value from the model for the samples\n",
    "y_test_forest = forest.predict(X_test)\n",
    "y_train_forest = forest.predict(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Performance Evaluation:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#computing the accuracy of the model performance\n",
    "acc_train_forest = accuracy_score(y_train,y_train_forest)\n",
    "acc_test_forest = accuracy_score(y_test,y_test_forest)\n",
    "\n",
    "print(\"Random forest: Accuracy on training Data: {:.3f}\".format(acc_train_forest))\n",
    "print(\"Random forest: Accuracy on test Data: {:.3f}\".format(acc_test_forest))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#checking the feature improtance in the model\n",
    "plt.figure(figsize=(9,7))\n",
    "n_features = X_train.shape[1]\n",
    "plt.barh(range(n_features), forest.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_features), X_train.columns)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Storing the results:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#storing the results. The below mentioned order of parameter passing is important.\n",
    "#Caution: Execute only once to avoid duplications.\n",
    "storeResults('Random Forest', acc_train_forest, acc_test_forest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Sorting the datafram on accuracy\n",
    "results.sort_values(by=['Test Accuracy', 'Train Accuracy'], ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the above comparision, Random forests works well with this dataset.\n",
    "\n",
    "So, saving the model for future use.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}